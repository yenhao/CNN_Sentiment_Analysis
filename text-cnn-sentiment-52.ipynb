{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def filter_tweet(tweet, token = tknzr):\n",
    "    tweet = re.sub('https?:\\/\\/\\S+','URLTOK',tweet.lower().strip()) # url\n",
    "    tweet = re.sub('@(?:[a-zA-Z]+|[0-9 \\/]+)', 'USRTOK', tweet) # mention\n",
    "    tweet = re.sub('(\\:|\\=)(?:\\)|\\-|\\(|D| )+', '', tweet) # emoticon\n",
    "    return token.tokenize(tweet) # return tokenized\n",
    "\n",
    "# transform the tweet sentence to numerical representation\n",
    "def word_transform(tweet_set, max_length = 60):\n",
    "    set_array = []\n",
    "    for tweet in tweet_set:\n",
    "        tweet_array = [0] * max_length\n",
    "        for i, word in enumerate(tweet):\n",
    "            if vocabulary_dict.get(word):\n",
    "                tweet_array[i] = vocabulary_dict[word]\n",
    "            else:\n",
    "                tweet_array[i] = 0\n",
    "        set_array.append(tweet_array)\n",
    "    return np.array(set_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Training data\n",
    "en_train = pd.read_csv('data/supervised_phase/en_full/en_full.tsv', delimiter='\\t', names=[\"id\", \"lang\", \"polarity\", \"tweet\"])\n",
    "en_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11378</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11379</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11380</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11381</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11382</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id lang  polarity                                              tweet\n",
       "0  11378   en   neutral  Picturehouse's, Pink Floyd's, 'Roger Waters: T...\n",
       "1  11379   en   neutral  Order Go Set a Watchman in store or through ou...\n",
       "2  11380   en  negative  If these runway renovations at the airport pre...\n",
       "3  11381   en   neutral  If you could ask an onstage interview question...\n",
       "4  11382   en  positive  A portion of book sales from our Harper Lee/Go..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Testing data\n",
    "en_test = pd.read_csv('data/supervised_phase/en_full/en_test.tsv', delimiter='\\t', names=[\"id\", \"lang\", \"polarity\", \"tweet\"])\n",
    "en_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Gas by my house hit $3.39!!!! I'm going to Cha...\n",
       "1    Theo Walcott is still shit, watch Rafa and Joh...\n",
       "2    its not that I'm a GSP fan, i just hate Nick D...\n",
       "3    Iranian general says Israel's Iron Dome can't ...\n",
       "4    with J Davlar 11th. Main rivals are team Polan...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_full = pd.concat([en_train['tweet'],en_test['tweet']])\n",
    "en_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to index sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_token(tweet):\n",
    "    \"\"\"\n",
    "    Convert a tweet to a sequence of word index \n",
    "    \"\"\"\n",
    "    return [token for token in tknzr.tokenize(tweet.lower())]\n",
    "\n",
    "def polarity2label(polarity):\n",
    "    if polarity =='negative':  return 0\n",
    "    elif polarity =='neutral': return 1\n",
    "    elif polarity =='positive':return 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token = en_train['tweet'].map(sentence_token)\n",
    "test_token = en_test['tweet'].map(sentence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66386"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "vocabulary_dict = {}\n",
    "word_index = 0\n",
    "\n",
    "for tweet in list(test_token) + list(train_token):\n",
    "    for word in tweet:\n",
    "        if word not in vocabulary_dict:\n",
    "            vocabulary_dict[word] = word_index\n",
    "            word_index +=1\n",
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit, watch Rafa and Joh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I'm a GSP fan, i just hate Nick D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id lang  polarity  \\\n",
       "0  264183816548130816   en  positive   \n",
       "1  263405084770172928   en  negative   \n",
       "2  262163168678248449   en  negative   \n",
       "3  264249301910310912   en  negative   \n",
       "4  264105751826538497   en  positive   \n",
       "\n",
       "                                               tweet  label  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...      2  \n",
       "1  Theo Walcott is still shit, watch Rafa and Joh...      0  \n",
       "2  its not that I'm a GSP fan, i just hate Nick D...      0  \n",
       "3  Iranian general says Israel's Iron Dome can't ...      0  \n",
       "4  with J Davlar 11th. Main rivals are team Polan...      2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en_full['index_rep'] = en_full['tweet'].map(word2index)\n",
    "en_train['label'] = en_train['polarity'].map(polarity2label)\n",
    "en_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11378</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Picturehouse's, Pink Floyd's, 'Roger Waters: T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11379</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Order Go Set a Watchman in store or through ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11380</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>If these runway renovations at the airport pre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11381</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If you could ask an onstage interview question...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11382</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>A portion of book sales from our Harper Lee/Go...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id lang  polarity                                              tweet  \\\n",
       "0  11378   en   neutral  Picturehouse's, Pink Floyd's, 'Roger Waters: T...   \n",
       "1  11379   en   neutral  Order Go Set a Watchman in store or through ou...   \n",
       "2  11380   en  negative  If these runway renovations at the airport pre...   \n",
       "3  11381   en   neutral  If you could ask an onstage interview question...   \n",
       "4  11382   en  positive  A portion of book sales from our Harper Lee/Go...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_test['label'] = en_test['polarity'].map(polarity2label)\n",
    "en_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(tknzr.tokenize(tweet)) for tweet in  en_full])\n",
    "# max_document_length = 44\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length).fit(en_full)\n",
    "\n",
    "tweets_train = np.array(list(vocab_processor.transform(en_train['tweet'])))\n",
    "tweets_test  = np.array(list(vocab_processor.transform(en_test['tweet'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18044, 53)\n",
      "(20632, 53)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 13, 27,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweets_train.shape)\n",
    "print(tweets_test.shape)\n",
    "tweets_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18044, 60)\n",
      "(20632, 60)\n"
     ]
    }
   ],
   "source": [
    "max_length = 60\n",
    "tweets_train = word_transform(train_token, max_length = max_length)\n",
    "print(tweets_train.shape)\n",
    "tweets_test = word_transform(test_token, max_length = max_length)\n",
    "print(tweets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18044, 60)\n",
      "(20632, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([38816, 27262,    14,   742,  3757,     1,    19, 27737,    39,\n",
       "        1468,  1153,   117,   477,    21,   488,    18,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweets_train.shape)\n",
    "print(tweets_test.shape)\n",
    "tweets_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training + Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import ShuffleSplit\n",
    "# rs = ShuffleSplit(n_splits=1, test_size=.1, random_state=0)\n",
    "\n",
    "senti_train = en_train['label'].as_matrix()\n",
    "senti_test  = en_test['label'].as_matrix()\n",
    "\n",
    "# for train_index, test_index in rs.split(senti):\n",
    "#     X_train = tweets[train_index]\n",
    "#     y_train = senti[train_index]\n",
    "\n",
    "#     X_test = tweets[test_index]\n",
    "#     y_test = senti[test_index]\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrain Word2Vec Model (#DIM 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9770612, 52)\n",
      "9770611\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load pre train Word2vec\n",
    "wb_matrix = np.load(\"data/embedding/en_word2vec_52.npy\")\n",
    "print(wb_matrix.shape)\n",
    "vocabulary_dict_ = pickle.load(open(\"data/embedding/vocabulary_dict_52.pickle\", \"rb\"))\n",
    "print(len(vocabulary_dict_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924\n",
      "246\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_dict.get('hi'))\n",
    "print(vocabulary_dict_.get('hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66386, 52)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # initial matrix with random uniform\n",
    "initW = np.random.uniform(-0.25,0.25,(len(vocabulary_dict), wb_matrix.shape[1]))\n",
    "# load any vectors from the word2vec\n",
    "# for word, vector in vocabulary_dict.items():\n",
    "#     idx = vocab_processor.vocabulary_.get(word)\n",
    "#     if idx != 0:\n",
    "#         initW[idx] = wb_matrix[vector]\n",
    "# initW.shape\n",
    "for word, index in vocabulary_dict.items():\n",
    "#     print(word)\n",
    "#     print(index)\n",
    "    idx = vocabulary_dict_.get(word)\n",
    "#     print(idx)\n",
    "    \n",
    "    if idx != 0 and idx is not None:\n",
    "        initW[index] = wb_matrix[idx]\n",
    "        \n",
    "initW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del wb_matrix, vocabulary_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = max_length\n",
    "n_output = 3\n",
    "learning_rate = 1\n",
    "\n",
    "embedding_size = initW.shape[1]\n",
    "filter_sizes = [4,3]\n",
    "num_filters = 200\n",
    "pooling_size = 4\n",
    "pooling_strides = 2\n",
    "epochs_num = 25\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 27, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, shape = (None, n_input), name = \"Input_X\")\n",
    "y = tf.placeholder(tf.int32, shape = (None), name = \"Y\")\n",
    "# mode = tf.placeholder(tf.bool, name = \"Mode\")\n",
    "\n",
    "# Load Embedding Model\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "    word2vec = tf.Variable(tf.constant(0.0, shape = initW.shape),\n",
    "                    trainable=False, name=\"word2vec\") # trainable=False, means not update these embeddings\n",
    "\n",
    "embedded_chars = tf.nn.embedding_lookup(word2vec, X)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1) # ex: convert [[1,2]] to [[1],[2]], that is shape (2,) to (2,1)\n",
    "\n",
    "# 1st convolution layer\n",
    "conv1 = tf.layers.conv2d(embedded_chars_expanded, \n",
    "                         filters = num_filters, \n",
    "                         kernel_size = (filter_sizes[0], initW.shape[1]),\n",
    "                         strides = (1,1), \n",
    "#                          padding=\"same\",\n",
    "                         kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                         \n",
    "                         activation = tf.nn.relu,\n",
    "                         name=\"Convolution_1st\"\n",
    "                        )\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[4, 1], strides=2)\n",
    "\n",
    "# 2nd convolution layer\n",
    "conv2 = tf.layers.conv2d(pool1, \n",
    "                         filters = num_filters, \n",
    "                         kernel_size = (filter_sizes[1], 1),\n",
    "                         strides = (1,1), \n",
    "                         padding=\"same\",\n",
    "                         kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                         activation = tf.nn.relu,\n",
    "                         name=\"Convolution_2nd\"\n",
    "                        )\n",
    "print(conv2.shape)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[27, 1], strides=1)\n",
    "\n",
    "# Dense Layer, Combine all the pooled features\n",
    "pool2_flat = tf.reshape(pool2, [-1, num_filters])\n",
    "\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu, name = \"Fully_connect\")\n",
    "# dense = tf.layers.dropout(inputs = dense, rate = 0.3, training = mode)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dense, \n",
    "                         units=n_output, \n",
    "                         activation=tf.nn.softmax, \n",
    "#                          kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                         name = \"Softmax\")\n",
    "\n",
    "# Define Loss Function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name=\"Cross_Entropy\"))\n",
    "\n",
    "# Define Training Process\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate, epsilon=1e-6).minimize(cross_entropy)\n",
    "\n",
    "# Define Accuracy\n",
    "predicted_class = tf.argmax(logits,1, output_type=tf.int32)\n",
    "correct_predict = tf.equal(y, predicted_class) # [True, False ..., True]\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32)) # [True, False ..., True] --> [1,0,...,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     show_graph(tf.get_default_graph().as_graph_def())\n",
    "#     # Initialize all variables\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     sess.run(word2vec.assign(initW))# Assign the pretrain word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the training batch\n",
    "train_data = tf.contrib.data.Dataset.from_tensor_slices((tweets_train, senti_train)).batch(batch_size).repeat()\n",
    "train_iterator = train_data.make_one_shot_iterator() # Create an iterator to go through the training data\n",
    "train_next_batch = train_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yenhao/Documents/chatbot/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-24 17:19:48 -    1 epoch, loss:0.874, train accuracy:0.672, train f1 score:0.540, test f1 score:0.193\n",
      "2017-11-24 17:19:49 -    2 epoch, loss:0.964, train accuracy:0.562, train f1 score:0.483, test f1 score:0.383\n",
      "2017-11-24 17:19:50 -    3 epoch, loss:0.971, train accuracy:0.547, train f1 score:0.496, test f1 score:0.405\n",
      "2017-11-24 17:19:51 -    4 epoch, loss:0.838, train accuracy:0.719, train f1 score:0.647, test f1 score:0.451\n",
      "2017-11-24 17:19:51 -    5 epoch, loss:0.806, train accuracy:0.734, train f1 score:0.636, test f1 score:0.469\n",
      "2017-11-24 17:19:52 -    6 epoch, loss:0.979, train accuracy:0.562, train f1 score:0.458, test f1 score:0.471\n",
      "2017-11-24 17:19:53 -    7 epoch, loss:0.768, train accuracy:0.812, train f1 score:0.777, test f1 score:0.538\n",
      "2017-11-24 17:19:54 -    8 epoch, loss:1.028, train accuracy:0.531, train f1 score:0.449, test f1 score:0.511\n",
      "2017-11-24 17:19:55 -    9 epoch, loss:0.892, train accuracy:0.672, train f1 score:0.640, test f1 score:0.518\n",
      "2017-11-24 17:19:55 -   10 epoch, loss:0.860, train accuracy:0.672, train f1 score:0.563, test f1 score:0.490\n",
      "2017-11-24 17:19:56 -   11 epoch, loss:0.970, train accuracy:0.531, train f1 score:0.548, test f1 score:0.362\n",
      "2017-11-24 17:19:57 -   12 epoch, loss:0.762, train accuracy:0.797, train f1 score:0.798, test f1 score:0.428\n",
      "2017-11-24 17:19:58 -   13 epoch, loss:0.796, train accuracy:0.766, train f1 score:0.760, test f1 score:0.480\n",
      "2017-11-24 17:19:59 -   14 epoch, loss:0.875, train accuracy:0.641, train f1 score:0.629, test f1 score:0.584\n"
     ]
    }
   ],
   "source": [
    "round_of_epochs = int(tweets_train.shape[0]/batch_size)\n",
    "\n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "\n",
    "train_f1 = []\n",
    "test_f1 = []\n",
    "x_axis = np.arange(0., epochs_num, 1)\n",
    "saver = tf.train.Saver() # to store the model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sess.run(word2vec.assign(initW))# Assign the pretrain word2vec\n",
    "    \n",
    "    for epochs in range(epochs_num): # starting the training process and set the epochs_num\n",
    "        for _ in range(round_of_epochs):\n",
    "#             print('_')\n",
    "            train, label = sess.run(train_next_batch) # Get the mini-batch data sample\n",
    "            sess.run(train_step, feed_dict={X:train, y:label}) # Feed the features, labe, training_mode  to network to train\n",
    "#         print(epochs)\n",
    "#         if epochs % 5 ==0:\n",
    "        loss, pred, acc = sess.run([cross_entropy,predicted_class,accuracy], feed_dict={X:train, y:label})\n",
    "        t_loss, t_pred = sess.run([cross_entropy,predicted_class], feed_dict={X:tweets_test, y:senti_test})\n",
    "        train_loss.append(loss)\n",
    "        test_loss.append(t_loss)\n",
    "        train_f1.append(f1_score(label, pred, average='weighted'))\n",
    "        test_f1.append(f1_score(senti_test, t_pred, average='weighted'))\n",
    "#         if epochs % 10 ==0:\n",
    "        print(\"{} - {:4d} epoch, loss:{:.3f}, train accuracy:{:.3f}, train f1 score:{:.3f}, test f1 score:{:.3f}\".format(\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            epochs+1,\n",
    "            loss, \n",
    "            acc,\n",
    "            f1_score(label, pred, average='weighted'),\n",
    "            f1_score(senti_test, t_pred, average='weighted')\n",
    "            )\n",
    "        )\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.plot(x_axis, train_loss, 'r', x_axis, test_loss, 'b')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss(cross entropy)')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.plot(x_axis, train_f1, x_axis, test_f1)\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss(cross entropy)')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
